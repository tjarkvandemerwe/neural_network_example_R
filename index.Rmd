---
# title: "Neural Network Example in R"
output: 
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

### Goal
The goal is to create a working example of a basic deep neural network in R.
This example uses the tidyverse style of R. The main goal is clarity and explaining the concept, not speed or practical application.

### Packages
```{r library, message = FALSE, warning = FALSE}
library(tidyverse)
```


### Training data
The training data used in this example is fake data. And this example starts of with an output that is lineairly related to two input variables.
```{r fake_data, message = FALSE, warning = FALSE}
data <- tibble(
  input_value_1 = c(0:10) / 10,
  input_value_2 = c(10:0) / 50,
  output_value = c(0:10) / -5 + 1
)

data |> head(7)
```


### Setup neural network
```{r nn_parameters, message = FALSE, warning = FALSE}
input_neurons  <- 2
output_neurons <- 1
layers         <- 2
layer_neurons  <- 3
```

Every layer, except for the input layer, has a weigth matrix and bias vector associated with it. These are also related to the computational steps when passing the signal forward. So the structure to store these parameters will look like this:

```{r nn_datastructure, echo = FALSE}
list(
  first_step = list(
    weigths = matrix(1, nrow = 2, ncol = 2),
    biases = rep(1, 2)
  ),
  second_step = list(
    weigths = matrix(1, nrow = 2, ncol = 2),
    biases = rep(1, 2)
  )
)
```

First a matrix of all weights (w) from the input neurons to the first layer of hidden neurons is created. This is an input_neurons x layer_neurons matrix, where input_neurons is the amount of neurons in the input layer, and layer_neurons is the amount of neurons per hidden layer. And this is repeated until the last layer to the output neurons is innitialised.

```{r nn_w_and_b, message = FALSE, warning = FALSE}
neurons_per_layer <- c(input_neurons, rep(layer_neurons, layers), output_neurons)

create_weights_and_biases <- function(neurons_from, neurons_to) {
  list(
    weights = matrix(
      runif(neurons_from * neurons_to, -1, 1),
      nrow = neurons_to,
      dimnames = list(
        paste0("to_", 1:neurons_to),
        paste0("from_", 1:neurons_from)
      )
    ),
    biases = runif(neurons_to, -1, 1)
  )
}

weights_and_biases <-
  map2(neurons_per_layer[-length(neurons_per_layer)],
       neurons_per_layer[-1],
       create_weights_and_biases)

names(weights_and_biases) <-
  paste0("step_", 1:length(weights_and_biases))

weights_and_biases

```



# Forward pass

To pass the signal through the network we need to iterate over the layers in the network, following the steps:
- Calculate weigthed sum of inputs to each neuron
- Minus bias of neuron
- Activation function

To do this three functions are build.

### Weighted sum activation
This function computes the weighted sum of all activations leading to this layer.

```{r weighted_sum, message = FALSE, warning = FALSE}

weighted_activation <- function(input_activation, weights) {
  weights %*% input_activation
}

# Example:
weights_and_biases$step_1$weights
weighted_activation(c(1, 0.5), weights_and_biases$step_1$weights)

```
### Reduce activation with bias
The bias is removed form the activation for each neuron in the layer. This acts as a sort of treshhold.

```{r biases_correct, message = FALSE, warning = FALSE}

remove_bias <- function(basic_activations, biases) {
  basic_activations - biases
}

# Example:
c(1, 0.5) |>
  weighted_activation(weights_and_biases$step_1$weights) |>
  remove_bias(c(-1, 0, 1))

```
### Apply activation function
For each neuron the value after the bias is removed, is put into an activation function.
This is the final value for the activation of this layer, and provides the input for the next layer.

```{r sigmoid, message = FALSE, warning = FALSE}

sigmoid <- function(basic_activations) {
  1 / (1 + exp(-basic_activations))
}

# Example:
c(1, 0.5) |>
  weighted_activation(weights_and_biases$step_1$weights) |>
  remove_bias(c(-1, 0, 1)) |> 
  sigmoid()

```

### Total forward pass

The total feed forward pass apply's the above three functions in a sequence. To make it easy a new function is build applying the above three steps.

```{r activate_layer, message = FALSE, warning = FALSE}

activate_layer <- function(input_activations, weights_and_biases) {
  input_activations |>
    weighted_activation(weights_and_biases$weights) |>
    remove_bias(weights_and_biases$biases) |>
    sigmoid()
}

# Now we want to iteratively apply this function, carrying the activation forward, for example:
c(1, 0.5) |>
  activate_layer(weights_and_biases$step_1) |>
  activate_layer(weights_and_biases$step_2) |>
  activate_layer(weights_and_biases$step_3)

# this can be made more simple using the reduce function:
reduce(weights_and_biases, activate_layer, .init = c(0.5, 1))

```
This concludes the example of the forward pass.


# Back propagation